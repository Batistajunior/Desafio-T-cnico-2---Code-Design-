{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3408d2-c2c8-44e6-b169-ce87e9cb961f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": ""
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "b3febec1-b61810d5bb2e43cd80c71072"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a3b0e3-83bb-41cb-af9c-76f4657317c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------+\n|\"COD_ID_CLIENTE\";\"DES_TIPO_CLIENTE\";\"NOM_NOME\";\"DES_SEXO_CLIENTE\";\"DAT_DATA_NASCIMENTO\"|\n+---------------------------------------------------------------------------------------+\n|\"1\";\"J\";\"COMERCIAL KAREN MATHIS\";\"\";\"\"                                                 |\n|\"2\";\"J\";\"COMERCIAL MICHAEL MORGAN\";\"\";\"\"                                               |\n|\"3\";\"J\";\"COMERCIAL ERICA LINDSEY\";\"\";\"\"                                                |\n|\"5\";\"J\";\"COMERCIAL KENNETH SHAW\";\"\";\"\"                                                 |\n|\"9\";\"J\";\"COMERCIAL JEREMY WILLIAMS\";\"\";\"\"                                              |\n|\"10\";\"J\";\"COMERCIAL JAMES BARBER\";\"\";\"\"                                                |\n|\"12\";\"J\";\"COMERCIAL THERESA DELEON\";\"\";\"\"                                              |\n|\"15\";\"J\";\"COMERCIAL KAYLA REID\";\"\";\"\"                                                  |\n|\"16\";\"J\";\"COMERCIAL STEPHEN BATES\";\"\";\"\"                                               |\n|\"17\";\"J\";\"COMERCIAL ROBERT MORRIS\";\"\";\"\"                                               |\n|\"18\";\"J\";\"COMERCIAL MONICA GREEN\";\"\";\"\"                                                |\n|\"19\";\"J\";\"COMERCIAL JENNIFER SHARP\";\"\";\"\"                                              |\n|\"50\";\"J\";\"COMERCIAL MR. BRIAN LANE DDS\";\"\";\"\"                                          |\n|\"60\";\"J\";\"COMERCIAL ASHLEY CHANDLER\";\"\";\"\"                                             |\n|\"80\";\"J\";\"COMERCIAL MIGUEL ALI\";\"\";\"\"                                                  |\n|\"100\";\"J\";\"COMERCIAL MATTHEW MCDANIEL\";\"\";\"\"                                           |\n|\"101\";\"J\";\"ASTORIA KATHERINE HENDERSON\";\"\";\"\"                                          |\n|\"102\";\"F\";\"JHONATAN JOSHUA JAMES\";\"\";\"\"                                                |\n|\"104\";\"J\";\"LIMPPANO WILLIAM KRUEGER\";\"\";\"\"                                             |\n|\"106\";\"F\";\"JOAO JAMES STUART\";\"M\";\"\"                                                   |\n+---------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#clientes = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/tables/clientes_csv.gz\")\n",
    "#clientes = clientes.select(col(\"COD_ID_CLIENTE\"), col(\"DAT_DATA_NASCIMENTO\"))\n",
    "clientes_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/tables/clientes_csv.gz\")\n",
    "clientes_df = clientes_df.dropna()\n",
    "clientes_df.show(truncate=False)\n",
    "\n",
    "#clientes_df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde50148-39a1-4a0d-b6e8-af9e5e807b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------------+----------------+-------------------+\n|COD_ID_CLIENTE|DES_TIPO_CLIENTE|NOM_NOME                |DES_SEXO_CLIENTE|DAT_DATA_NASCIMENTO|\n+--------------+----------------+------------------------+----------------+-------------------+\n|131           |F               |ALTAMIR JORDAN SIMMONS  |M               |1974-08-09         |\n|193           |F               |ADRIANO KENDRA JOHNSON  |M               |1979-09-04         |\n|248           |F               |JOSE CARRIE CUNNINGHAM  |M               |1962-01-26         |\n|275           |F               |GUSTAVO JENNIFER HIGGINS|M               |1979-04-14         |\n|407           |F               |DANILO CHRISTINE DIXON  |M               |1951-11-05         |\n|417           |F               |JOSE SHARON HERRERA     |M               |1971-10-28         |\n|336           |F               |MARIA DENISE RODGERS    |F               |1971-09-10         |\n|420           |F               |MARCI TARA JOHNSON      |F               |1971-06-27         |\n|421           |F               |MARCIO ADAM RAMIREZ     |M               |1979-07-31         |\n|424           |F               |PAULO NICOLE HOWELL     |M               |1965-12-16         |\n|425           |F               |PEDRO NICHOLAS BEAN     |M               |1950-02-06         |\n|426           |F               |PEDRO CRYSTAL MARTINEZ  |M               |1968-11-05         |\n|427           |F               |RODRIGO CHRISTINA GLENN |M               |1982-08-06         |\n|454           |F               |TEONISIA PATRICK EVANS  |F               |1960-02-14         |\n|509           |F               |SERGIO ANDREW RICE      |M               |1964-08-30         |\n|560           |F               |RODRIGO MICHELLE MOORE  |M               |1976-10-26         |\n|843           |F               |LUIZ ANDRE GILLESPIE    |M               |1984-04-11         |\n|964           |F               |DOUGLAS BRIANNA THOMAS  |M               |1986-06-05         |\n|1087          |F               |DIEGO WENDY HOBBS       |M               |1985-05-18         |\n|1092          |F               |RONALDO JEREMIAH GUZMAN |M               |1980-11-14         |\n+--------------+----------------+------------------------+----------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Carrega o arquivo csv.gz para um DataFrame\n",
    "clientes_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"dbfs:/FileStore/tables/clientes_csv.gz\")\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna COD_ID_CLIENTE\n",
    "clientes_df = clientes_df.withColumn(\"COD_ID_CLIENTE\", regexp_replace(\"COD_ID_CLIENTE\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DES_TIPO_CLIENTE\n",
    "clientes_df = clientes_df.withColumn(\"DES_TIPO_CLIENTE\", regexp_replace(\"DES_TIPO_CLIENTE\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna NOM_NOME\n",
    "clientes_df = clientes_df.withColumn(\"NOM_NOME\", regexp_replace(\"NOM_NOME\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DES_SEXO_CLIENTE\n",
    "clientes_df = clientes_df.withColumn(\"DES_SEXO_CLIENTE\", regexp_replace(\"DES_SEXO_CLIENTE\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DAT_DATA_NASCIMENTO\n",
    "clientes_df = clientes_df.withColumn(\"DAT_DATA_NASCIMENTO\", regexp_replace(\"DAT_DATA_NASCIMENTO\", '\"', \"\"))\n",
    "\n",
    "\n",
    "# Remove as linhas que contém valores nulos\n",
    "clientes_df = clientes_df.na.drop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove as linhas que contém valores nulos\n",
    "clientes_df = clientes_df.dropna()\n",
    "clientes_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50fbe11-b1f9-4e77-a445-60821c0a5e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------------+----------------+-------------------+\n|COD_ID_CLIENTE|DES_TIPO_CLIENTE|NOM_NOME                |DES_SEXO_CLIENTE|DAT_DATA_NASCIMENTO|\n+--------------+----------------+------------------------+----------------+-------------------+\n|131           |F               |ALTAMIR JORDAN SIMMONS  |M               |1974-08-09         |\n|193           |F               |ADRIANO KENDRA JOHNSON  |M               |1979-09-04         |\n|248           |F               |JOSE CARRIE CUNNINGHAM  |M               |1962-01-26         |\n|275           |F               |GUSTAVO JENNIFER HIGGINS|M               |1979-04-14         |\n|407           |F               |DANILO CHRISTINE DIXON  |M               |1951-11-05         |\n|417           |F               |JOSE SHARON HERRERA     |M               |1971-10-28         |\n|336           |F               |MARIA DENISE RODGERS    |F               |1971-09-10         |\n|420           |F               |MARCI TARA JOHNSON      |F               |1971-06-27         |\n|421           |F               |MARCIO ADAM RAMIREZ     |M               |1979-07-31         |\n|424           |F               |PAULO NICOLE HOWELL     |M               |1965-12-16         |\n|425           |F               |PEDRO NICHOLAS BEAN     |M               |1950-02-06         |\n|426           |F               |PEDRO CRYSTAL MARTINEZ  |M               |1968-11-05         |\n|427           |F               |RODRIGO CHRISTINA GLENN |M               |1982-08-06         |\n|454           |F               |TEONISIA PATRICK EVANS  |F               |1960-02-14         |\n|509           |F               |SERGIO ANDREW RICE      |M               |1964-08-30         |\n|560           |F               |RODRIGO MICHELLE MOORE  |M               |1976-10-26         |\n|843           |F               |LUIZ ANDRE GILLESPIE    |M               |1984-04-11         |\n|964           |F               |DOUGLAS BRIANNA THOMAS  |M               |1986-06-05         |\n|1087          |F               |DIEGO WENDY HOBBS       |M               |1985-05-18         |\n|1092          |F               |RONALDO JEREMIAH GUZMAN |M               |1980-11-14         |\n+--------------+----------------+------------------------+----------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "categoriasprodutos_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/categorias_produtos_csv.gz\")\n",
    "categoriasprodutos_df = clientes_df.dropna()\n",
    "categoriasprodutos_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80554302-09d3-49c8-ba69-7fea6e5c2b4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------+----------------------------------------+----------------------------------+-----------+-----------------+\n|COD_ID_PRODUTO|COD_ID_CATEGORIA_PRODUTO|ARR_CATEGORIAS_PRODUTO                  |DES_PRODUTO                       |DES_UNIDADE|COD_CODIGO_BARRAS|\n+--------------+------------------------+----------------------------------------+----------------------------------+-----------+-----------------+\n|21            |10652                   |['10639','9985','10620','10240','10652']|TALCO TRA LA LA 160G SUAVE BABY   |UN         |7896115700392    |\n|41            |9362                    |['9335','9234','9361','9355','9362']    |ABACATE FORTUNA KG                |KG         |3018             |\n|42            |9362                    |['9335','9234','9361','9355','9362']    |ABACATE AVOCADO KG                |KG         |76845            |\n|43            |9363                    |['9335','9234','9361','9355','9363']    |ABACAXI CAIANO UN                 |UN         |2100302          |\n|44            |9363                    |['9335','9234','9361','9355','9363']    |ABACAXI PEROLA UNID.              |UN         |35758884         |\n|49            |9400                    |['9335','9397','9234','9398','9400']    |ABOBRINHA COMPRIDA ITALIA KG      |KG         |3445             |\n|53            |9353                    |['9346','9343','9335','9234','9353']    |AGRIAO HIDROPONICA NATURALHIDRO UN|UN         |7898930624075    |\n|63            |9354                    |['9346','9343','9335','9234','9354']    |ALHO PORO UN                      |UN         |534307           |\n|64            |9338                    |['9336','9335','9234','9337','9338']    |ALHO SOLTO KG                     |KG         |9812             |\n|65            |9380                    |['9335','9234','9355','9376','9380']    |AMEIXA IMPORTADA KG               |KG         |3919             |\n|70            |9364                    |['9335','9234','9361','9355','9364']    |BANANA CATURRA KG                 |KG         |3049             |\n|72            |9364                    |['9335','9234','9361','9355','9364']    |BANANA PRATA KG                   |KG         |3056             |\n|73            |9418                    |['9417','9335','9397','9234','9418']    |BATATA BRANCA KG                  |KG         |7993             |\n|76            |9418                    |['9417','9335','9397','9234','9418']    |BATATA DOCE BRANCA KG             |KG         |3520             |\n|77            |9418                    |['9417','9335','9397','9234','9418']    |BATATA DOCE ROXA KG               |KG         |2448             |\n|78            |9418                    |['9417','9335','9397','9234','9418']    |BATATA ROSA KG                    |KG         |3544             |\n|80            |9357                    |['9335','9234','9356','9355','9357']    |BERGAMOTA MONTENEGRINA (CAI) KG   |KG         |3063             |\n|82            |9401                    |['9335','9397','9234','9398','9401']    |BERINJELA COMUM KG                |KG         |3551             |\n|85            |10950                   |['9335','9397','9234','9398','10950']   |BETERRABA KG                      |KG         |3568             |\n|88            |9353                    |['9346','9343','9335','9234','9353']    |BROCOLIS UN                       |UN         |711616           |\n+--------------+------------------------+----------------------------------------+----------------------------------+-----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "#produtos_df = clientes_df.dropna()\n",
    "\n",
    "\n",
    "# Carrega o arquivo csv.gz para um DataFrame\n",
    "produtos_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"dbfs:/FileStore/tables/produtos_csv.gz\")\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna COD_ID_CATEGORIA_PRODUTO\n",
    "produtos_df = produtos_df.withColumn(\"COD_ID_CATEGORIA_PRODUTO\", regexp_replace(\"COD_ID_CATEGORIA_PRODUTO\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna ARR_CATEGORIAS_PRODUTO\n",
    "produtos_df = produtos_df.withColumn(\"ARR_CATEGORIAS_PRODUTO\", regexp_replace(\"ARR_CATEGORIAS_PRODUTO\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DES_PRODUTO\n",
    "produtos_df = produtos_df.withColumn(\"DES_PRODUTO\", regexp_replace(\"DES_PRODUTO\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DES_UNIDADE\n",
    "produtos_df = produtos_df.withColumn(\"DES_UNIDADE\", regexp_replace(\"DES_UNIDADE\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "produtos_df = produtos_df.withColumn(\"COD_CODIGO_BARRAS\", regexp_replace(\"COD_CODIGO_BARRAS\", '\"', \"\"))\n",
    "\n",
    "# Remove a linha de cabeçalho\n",
    "#produtos_df = produtos_df.drop(0)\n",
    "\n",
    "# Remove as linhas que contém valores nulos\n",
    "produtos_df = produtos_df.dropna()\n",
    "\n",
    "produtos_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65903cf6-88e5-4449-a566-6610d1784dab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3274478127227177>\u001B[0m in \u001B[0;36m<cell line: 17>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;31m# Remove as aspas duplas dos valores da coluna COD_ID_CATEGORIA_PRODUTO\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mvendas_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprodutos_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"COD_ID_LOJA\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mregexp_replace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"COD_ID_LOJA\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'\"'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;31m# Remove as aspas duplas dos valores da coluna ARR_CATEGORIAS_PRODUTO\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `COD_ID_LOJA` cannot be resolved. Did you mean one of the following? [`COD_ID_PRODUTO`, `COD_CODIGO_BARRAS`, `DES_PRODUTO`, `DES_UNIDADE`, `COD_ID_CATEGORIA_PRODUTO`];\n'Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, COD_CODIGO_BARRAS#257, regexp_replace('COD_ID_LOJA, \", , 1) AS COD_ID_LOJA#618]\n+- Filter atleastnnonnulls(6, COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, COD_CODIGO_BARRAS#257)\n   +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, regexp_replace(COD_CODIGO_BARRAS#221, \", , 1) AS COD_CODIGO_BARRAS#257]\n      +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, regexp_replace(DES_UNIDADE#220, \", , 1) AS DES_UNIDADE#250, COD_CODIGO_BARRAS#221]\n         +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, regexp_replace(DES_PRODUTO#219, \", , 1) AS DES_PRODUTO#243, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n            +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, regexp_replace(ARR_CATEGORIAS_PRODUTO#218, \", , 1) AS ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#219, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n               +- Project [COD_ID_PRODUTO#216, regexp_replace(COD_ID_CATEGORIA_PRODUTO#217, \", , 1) AS COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#218, DES_PRODUTO#219, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n                  +- Relation [COD_ID_PRODUTO#216,COD_ID_CATEGORIA_PRODUTO#217,ARR_CATEGORIAS_PRODUTO#218,DES_PRODUTO#219,DES_UNIDADE#220,COD_CODIGO_BARRAS#221] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-3274478127227177>\u001B[0m in \u001B[0;36m<cell line: 17>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;31m# Remove as aspas duplas dos valores da coluna COD_ID_CATEGORIA_PRODUTO\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mvendas_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprodutos_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"COD_ID_LOJA\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mregexp_replace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"COD_ID_LOJA\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'\"'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;31m# Remove as aspas duplas dos valores da coluna ARR_CATEGORIAS_PRODUTO\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3325\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3326\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3327\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3329\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"DataFrame\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `COD_ID_LOJA` cannot be resolved. Did you mean one of the following? [`COD_ID_PRODUTO`, `COD_CODIGO_BARRAS`, `DES_PRODUTO`, `DES_UNIDADE`, `COD_ID_CATEGORIA_PRODUTO`];\n'Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, COD_CODIGO_BARRAS#257, regexp_replace('COD_ID_LOJA, \", , 1) AS COD_ID_LOJA#618]\n+- Filter atleastnnonnulls(6, COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, COD_CODIGO_BARRAS#257)\n   +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, regexp_replace(COD_CODIGO_BARRAS#221, \", , 1) AS COD_CODIGO_BARRAS#257]\n      +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, regexp_replace(DES_UNIDADE#220, \", , 1) AS DES_UNIDADE#250, COD_CODIGO_BARRAS#221]\n         +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, regexp_replace(DES_PRODUTO#219, \", , 1) AS DES_PRODUTO#243, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n            +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, regexp_replace(ARR_CATEGORIAS_PRODUTO#218, \", , 1) AS ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#219, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n               +- Project [COD_ID_PRODUTO#216, regexp_replace(COD_ID_CATEGORIA_PRODUTO#217, \", , 1) AS COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#218, DES_PRODUTO#219, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n                  +- Relation [COD_ID_PRODUTO#216,COD_ID_CATEGORIA_PRODUTO#217,ARR_CATEGORIAS_PRODUTO#218,DES_PRODUTO#219,DES_UNIDADE#220,COD_CODIGO_BARRAS#221] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `COD_ID_LOJA` cannot be resolved. Did you mean one of the following? [`COD_ID_PRODUTO`, `COD_CODIGO_BARRAS`, `DES_PRODUTO`, `DES_UNIDADE`, `COD_ID_CATEGORIA_PRODUTO`];\n'Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, COD_CODIGO_BARRAS#257, regexp_replace('COD_ID_LOJA, \", , 1) AS COD_ID_LOJA#618]\n+- Filter atleastnnonnulls(6, COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, COD_CODIGO_BARRAS#257)\n   +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, DES_UNIDADE#250, regexp_replace(COD_CODIGO_BARRAS#221, \", , 1) AS COD_CODIGO_BARRAS#257]\n      +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#243, regexp_replace(DES_UNIDADE#220, \", , 1) AS DES_UNIDADE#250, COD_CODIGO_BARRAS#221]\n         +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#236, regexp_replace(DES_PRODUTO#219, \", , 1) AS DES_PRODUTO#243, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n            +- Project [COD_ID_PRODUTO#216, COD_ID_CATEGORIA_PRODUTO#228, regexp_replace(ARR_CATEGORIAS_PRODUTO#218, \", , 1) AS ARR_CATEGORIAS_PRODUTO#236, DES_PRODUTO#219, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n               +- Project [COD_ID_PRODUTO#216, regexp_replace(COD_ID_CATEGORIA_PRODUTO#217, \", , 1) AS COD_ID_CATEGORIA_PRODUTO#228, ARR_CATEGORIAS_PRODUTO#218, DES_PRODUTO#219, DES_UNIDADE#220, COD_CODIGO_BARRAS#221]\n                  +- Relation [COD_ID_PRODUTO#216,COD_ID_CATEGORIA_PRODUTO#217,ARR_CATEGORIAS_PRODUTO#218,DES_PRODUTO#219,DES_UNIDADE#220,COD_CODIGO_BARRAS#221] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#vendas_df = clientes_df.dropna()\n",
    "#vendas_df.show(truncate=False)\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import pandas as pd\n",
    "\n",
    "#vendas_df = pd.read_csv(\"dbfs:/FileStore/tables/vendas*_csv.gz\", sep=\";\", quotechar='\"')\n",
    "\n",
    "\n",
    "#produtos_df = clientes_df.dropna()\n",
    "\n",
    "\n",
    "# Carrega o arquivo csv.gz para um DataFrame\n",
    "vendas_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"dbfs:/FileStore/tables/vendas*.csv.gz\")\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna COD_ID_CATEGORIA_PRODUTO\n",
    "vendas_df = produtos_df.withColumn(\"COD_ID_LOJA\", regexp_replace(\"COD_ID_LOJA\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna ARR_CATEGORIAS_PRODUTO\n",
    "vendas_df = produtos_df.withColumn(\"NUM_ANOMESDIA\", regexp_replace(\"NUM_ANOMESDIA\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DES_PRODUTO\n",
    "vendas_df = produtos_df.withColumn(\"DES_TIPO_CLIENTE\", regexp_replace(\"DES_TIPO_CLIENTE\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna DES_UNIDADE\n",
    "vendas_df = produtos_df.withColumn(\"DES_SEXO_CLIENTE\", regexp_replace(\"DES_SEXO_CLIENTE\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"COD_ID_VENDA_UNICO\", regexp_replace(\"COD_ID_VENDA_UNICO\", '\"', \"\"))\n",
    "\n",
    "  # Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"COD_ID_VENDA_UNICO\", regexp_replace(\"COD_ID_VENDA_UNICO\", '\"', \"\")) \n",
    " \n",
    " # Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"COD_ID_PRODUTO\", regexp_replace(\"COD_ID_PRODUTO\", '\"', \"\"))\n",
    "\n",
    "# Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"VAL_VALOR_DESCONTO\", regexp_replace(\"VAL_VALOR_DESCONTO\", '\"', \"\"))\n",
    "\n",
    " # Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"VAL_VALOR_SEM_DESC\", regexp_replace(\"VAL_VALOR_SEM_DESC\", '\"', \"\"))\n",
    "                                   \n",
    " # Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"VAL_VALOR_COM_DESC\", regexp_replace(\"VAL_VALOR_COM_DESC\", '\"', \"\"))                                 \n",
    " \n",
    " # Remove as aspas duplas dos valores da coluna COD_CODIGO_BARRAS\n",
    "vendas_df = produtos_df.withColumn(\"VAL_QUANTIDADE_KG\", regexp_replace(\"VAL_QUANTIDADE_KG\", '\"', \"\"))                                  \n",
    "\n",
    "                                   # Remove a linha de cabeçalho\n",
    "#produtos_df = produtos_df.drop(0)\n",
    "\n",
    "# Remove as linhas que contém valores nulos\n",
    "vendas_df = vendas_df.dropna()\n",
    "\n",
    "vendas_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d6c602-09f3-49ac-b509-c6bd93aec9bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CaseDesafio02",
   "notebookOrigID": 1852240251668234,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
